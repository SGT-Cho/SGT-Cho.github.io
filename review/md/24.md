LLaMA 모델 파인튜닝: 이론과 실습
2025년 1월 26일
## 1. 서론

대규모 언어 모델(LLM)의 발전으로 자연어 처리 분야는 혁신적인 변화를 맞이하고 있습니다. 그중에서도 Meta AI에서 개발한 **LLaMA**(Large Language Model Meta AI) 모델은 상대적으로 적은 파라미터 수로도 우수한 성능을 보이며, 연구자들과 개발자들의 주목을 받고 있습니다. 이러한 모델을 특정 작업이나 도메인에 맞게 최적화하는 과정이 **파인튜닝(Fine-Tuning)**입니다. 본 글에서는 LLaMA 모델의 파인튜닝에 대한 이론적 배경과 함께, 실제로 파인튜닝을 수행하는 방법을 예제를 통해 상세히 설명하겠습니다.

## 2. 파인튜닝이란?

**파인튜닝**은 사전 학습된 대규모 언어 모델을 특정 작업이나 도메인에 맞게 미세 조정하는 과정을 의미합니다. 일반적으로 LLM은 방대한 양의 텍스트 데이터를 기반으로 사전 학습되어 다양한 작업을 수행할 수 있는 능력을 갖추고 있습니다. 그러나 특정 작업에서 최적의 성능을 발휘하기 위해서는 해당 작업에 특화된 추가 학습이 필요합니다. 이를 통해 모델은 특정 도메인의 용어와 문맥을 더 잘 이해하고, 해당 분야에 맞는 정확한 결과를 생성할 수 있습니다.

## 3. LLaMA 모델 소개

**LLaMA**는 Meta AI에서 개발한 언어 모델로, GPT-3와 비교하여 더 적은 파라미터 수를 가지면서도 높은 성능을 보입니다. 이는 연구자들과 개발자들에게 더 적은 자원으로 효율적인 모델 학습과 활용을 가능하게 합니다. LLaMA 모델은 7B, 13B, 70B 등 다양한 크기로 제공되며, 각 모델은 파라미터 수에 따라 성능과 자원 요구 사항이 다릅니다.

## 4. 파인튜닝을 위한 준비

파인튜닝을 수행하기 위해서는 다음과 같은 준비가 필요합니다:

### 4.1. 환경 설정

파인튜닝 작업은 높은 연산 자원을 필요로 하므로, GPU를 활용하는 것이 권장됩니다. 로컬 환경에 GPU가 없다면, Google Colab Pro와 같은 클라우드 서비스를 활용할 수 있습니다.

### 4.2. 라이브러리 설치

파인튜닝에 필요한 주요 라이브러리는 다음과 같습니다:

- `transformers`: 다양한 사전 학습된 언어 모델을 손쉽게 활용할 수 있는 라이브러리입니다.
- `datasets`: 다양한 데이터셋을 쉽게 로드하고 전처리할 수 있도록 지원합니다.
- `peft`: 파라미터 효율적 파인튜닝(Parameter-Efficient Fine-Tuning)을 위한 라이브러리로, 대규모 언어 모델을 효율적으로 미세 조정할 수 있습니다.
- `bitsandbytes`: 모델 파라미터의 양자화를 통해 메모리 사용량을 절감할 수 있는 라이브러리입니다.
- `trl`: 트랜스포머 모델에 강화 학습을 적용할 수 있는 라이브러리입니다.

다음은 이러한 라이브러리들을 설치하는 명령어입니다:

```bash
pip install -U accelerate peft bitsandbytes transformers trl datasets
```

### 4.3. 데이터셋 준비

파인튜닝에 사용할 데이터셋을 준비해야 합니다. 데이터셋은 모델이 학습할 수 있는 형식으로 전처리되어야 하며, 일반적으로 입력과 출력 쌍으로 구성됩니다. 예를 들어, 뉴스 기사의 요약 작업을 위한 데이터셋은 기사 본문과 해당 요약문으로 이루어집니다.

## 5. 파인튜닝 단계별 실습

이제 실제로 LLaMA 모델을 파인튜닝하는 과정을 단계별로 살펴보겠습니다.

### 5.1. 라이브러리 임포트

먼저, 필요한 라이브러리를 임포트합니다:

```python
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer
```

### 5.2. 모델 및 데이터셋 설정

파인튜닝에 사용할 모델과 데이터셋을 설정합니다. 여기서는 한국어로 파인튜닝된 LLaMA 모델과 주가 증권 보고서 데이터셋을 예시로 사용하겠습니다:

```python
# 베이스 모델 설정
base_model = "beomi/Llama-3-Open-Ko-8B"

# 데이터셋 설정
dataset_name = "uiyong/gemini_result_kospi_0517_jsonl"

# 새로운 모델 이름
new_model_name = "Llama3-owen-Ko-3-8B-kospi"
```

### 5.3. GPU 환경 확인 및 설정

파인튜닝 작업은 높은 연산 자원을 필요로 하므로, GPU 환경을 확인하고 설정합니다:

```python
# GPU 사용 여부 확인
device = "cuda" if torch.cuda.is_available() else "cpu"
```

### 5.4. 모델 로드 및 양자화 설정

모델을 로드하고 양자화를 설정하여 메모리 사용량을 최적화합니다:

```python
# 양자화 설정
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# 모델 로드
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map="auto",
)
```

### 5.5. 토크나이저 설정

토크나이저를 로드하고 필요한 설정을 수행합니다:

```python
tokenizer = AutoTokenizer.from_pretrained(base_model, add_special_tokens=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
```

### 5.6. 프롬프트 템플릿 생성

파인튜닝에 사용할 프롬프트 템플릿을 생성합니다. LLaMA 모델의 경우, 특정한 프롬프트 형식을 따릅니다:

```python
def generate_prompts(example):
    prompt_list = []
    for i in range(len(example['document'])):
        prompt_list.append(
            f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>다음 글을 요약해주세요: {example['document'][i]}<|eot_id|><|start_header_id|>assistant<|end_header_id|> {example['summary'][i]}<|eot_id|>"""
        )
    return prompt_list
```

## 5.7. 데이터셋 전처리

모델 학습을 위해 데이터셋을 전처리합니다. 이전에 정의한 `generate_prompts` 함수를 사용하여 프롬프트를 생성하고, 이를 토큰화합니다.

```python
# 데이터셋 로드
from datasets import load_dataset

dataset = load_dataset("daekeun-ml/naver-news-summarization-ko")

# 프롬프트 생성 함수 정의
def generate_prompts(example):
    prompt_list = []
    for i in range(len(example['document'])):
        prompt_list.append(
            f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>다음 글을 요약해주세요: {example['document'][i]}<|eot_id|><|start_header_id|>assistant<|end_header_id|> {example['summary'][i]}<|eot_id|>"""
        )
    return prompt_list

# 프롬프트 생성
dataset = dataset.map(generate_prompts, batched=True, remove_columns=dataset.column_names)

# 토큰화 함수 정의
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# 데이터셋 토큰화
tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

위 코드는 데이터셋을 로드하고, 각 예제에 대해 프롬프트를 생성한 후, 이를 토큰화하여 모델 학습에 적합한 형식으로 변환합니다.

## 5.8. 파인튜닝 설정

파인튜닝을 위한 설정을 정의합니다. 여기에는 학습률, 배치 크기, 에포크 수 등의 하이퍼파라미터가 포함됩니다.

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)
```

위 설정은 모델의 학습 과정에서 사용할 하이퍼파라미터를 정의합니다. `output_dir`는 모델 체크포인트가 저장될 디렉토리이며, `evaluation_strategy`는 평가 빈도를 설정합니다. `learning_rate`는 학습률을, `per_device_train_batch_size`와 `per_device_eval_batch_size`는 각각 훈련 및 평가 시의 배치 크기를 지정합니다. `num_train_epochs`는 전체 데이터셋에 대해 학습을 반복할 횟수를 나타내며, `weight_decay`는 가중치 감쇠를 위한 값입니다. `logging_dir`는 학습 로그가 저장될 디렉토리입니다.

## 5.9. 트레이너 초기화 및 학습

트레이너를 초기화하고 모델 학습을 시작합니다.

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
)

# 모델 학습
trainer.train()
```

위 코드는 `Trainer` 객체를 초기화하고, 앞서 정의한 학습 인자(`training_args`), 모델, 훈련 데이터셋, 평가 데이터셋을 전달합니다. 그런 다음 `trainer.train()`을 호출하여 모델 학습을 시작합니다.

## 5.10. 모델 저장 및 평가

학습이 완료되면 모델을 저장하고, 평가를 수행합니다.

```python
# 모델 저장
trainer.save_model("./fine_tuned_model")

# 평가
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

위 코드는 학습된 모델을 지정된 디렉토리에 저장하고, 평가 데이터셋을 사용하여 모델의 성능을 평가합니다. `Perplexity`는 언어 모델의 성능을 측정하는 지표로, 값이 낮을수록 모델의 성능이 우수함을 나타냅니다.

## 6. 결론

본 글에서는 LLaMA 모델을 특정 작업에 맞게 파인튜닝하는 전체 과정을 단계별로 살펴보았습니다. 파인튜닝을 통해 사전 학습된 대규모 언어 모델을 특정 도메인이나 작업에 최적화하여 더욱 우수한 성능을 달성할 수 있습니다. 이러한 과정을 통해 다양한 분야에서 LLaMA 모델을 효과적으로 활용할 수 있을 것으로 기대됩니다. 