Llama에 RAG를 적용하여 똑똑한 인공지능 만들기
2025년 1월 26일

### **1. 기본 준비 사항**
- **Ollama 설치**: [공식 문서](https://ollama.ai/) 참조
- **LLaMA3 모델 로드**:
  ```bash
  ollama pull llama3
  ```
- **필요 라이브러리**:
  ```python
  !pip install langchain faiss-cpu sentence-transformers
  ```

---

### **2. RAG 시스템 아키텍처**
```mermaid
flowchart LR
    A[사용자 질문] --> B[질문 임베딩]
    B --> C[벡터DB 유사도 검색]
    C --> D[관련 문서 청크 추출]
    D --> E[LLaMA3 답변 생성]
```

---

### **3. 단계별 구현 (저작권 주의사항 포함)**

#### **1. 데이터 준비**
- **✅ 허용 데이터**: 
  - 자체 제작 문서
  - CC-BY, MIT License 등 **오픈 라이선스** 자료
  - 위키백과(CC BY-SA 3.0)
- **❌ 금지 데이터**:
  - 저작권이 있는 서적/논문
  - 웹 스크래핑 데이터(명시적 허락 없을 시)

#### **2. 문서 청킹 & 임베딩**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

# 문서 분할
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,  # 청크 크기 조정
    chunk_overlap=50
)
chunks = text_splitter.split_documents(your_own_data)  # 반드시 자체 데이터 사용

# 임베딩 생성
embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
```

#### **3. 벡터 DB 구축**
```python
from langchain.vectorstores import FAISS

# FAISS 벡터 저장소 생성
vector_db = FAISS.from_documents(
    documents=chunks,
    embedding=embedder
)
vector_db.save_local("your_custom_index")  # 고유한 인덱스 이름 사용
```

#### **4. 검색 & 생성 파이프라인**
```python
from langchain.chains import RetrievalQA
from langchain.llms import Ollama

# 커스텀 프롬프트 템플릿
prompt_template = """
[INST] 다음 컨텍스트를 기반으로 질문에 답하세요:
{context}

질문: {question} 
답변은 3문장 이내로 작성하고, 출처를 명시하지 마세요 [/INST]
"""

# RAG 체인 구성
llm = Ollama(model="llama3")
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={"prompt": prompt_template}
)
```

---

### **4. 실행 예시 (자체 데이터 기반)**
```python
# 사용자 정의 질의응답
question = "우리 회사 제품의 주요 기술은 무엇인가요?"  # 실제 자사 데이터로 테스트
response = rag_chain.run(question)
print(response)
```

---

### **5. 주의사항**
1. **데이터 소싱**: 반드시 사용 권한이 확인된 데이터만 활용
2. **출력 검증**: 생성 결과가 특정 저작물을 복제하지 않도록 확인
3. **임베딩 모델**: `intfloat/e5-mistral-7b-instruct` 등 오픈소스 모델 사용 권장
4. **모델 튜닝**: LoRA를 활용해 자체 데이터로 미세조정 시 성능 향상 가능

> 💡 **참고**: RAG 성능은 데이터 품질에 크게 의존합니다. 자체 지식베이스를 체계적으로 구축하는 것이 가장 중요합니다.