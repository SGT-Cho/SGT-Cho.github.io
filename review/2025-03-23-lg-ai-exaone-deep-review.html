<!DOCTYPE html>
<html lang="ko" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LG AI Exaone deep 사용 후기 | Minjae's Life & Review Blog</title>
  
  <!-- Meta tags for SEO and social sharing -->
  <meta name="description" content="LG AI Research에서 출시한 Exaone deep 모델 사용 경험과 성능 분석 리뷰">
  <meta name="keywords" content="AI, LLM, exaone, lg ai, 인공지능, 리뷰">
  <meta name="author" content="Minjae Cho">
  
  <!-- Open Graph meta tags for social sharing -->
  <meta property="og:title" content="LG AI Exaone deep 사용 후기 | Minjae's Life & Review Blog">
  <meta property="og:description" content="LG AI Research에서 출시한 Exaone deep 모델 사용 경험과 성능 분석 리뷰">
  <meta property="og:image" content="/assets/images/portfolio/llm.png">
  <meta property="og:url" content="https://sgt-cho.github.io/review/2025-03-23-lg-ai-exaone-deep-review.html">
  <meta property="og:type" content="article">
  
  <!-- Twitter card meta tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="LG AI Exaone deep 사용 후기">
  <meta name="twitter:description" content="LG AI Research에서 출시한 Exaone deep 모델 사용 경험과 성능 분석 리뷰">
  <meta name="twitter:image" content="/assets/images/portfolio/llm.png">
  
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;600&display=swap" rel="stylesheet">
  
  <!-- Stylesheets -->
  <link rel="stylesheet" href="/css/style.css">
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="/assets/images/minjae.png">
  
  <!-- Web App Manifest -->
  <link rel="manifest" href="/manifest.json">
</head>
<body>
  <!-- 
    title: LG AI Exaone deep 사용 후기
    date: 2025-03-23
    category: review
    tags: AI, LLM, 리뷰, Korean-LLM
    teaser: LG AI Research에서 출시한 Exaone deep 모델 사용 경험과 성능 분석 리뷰
    thumbnail: llm.png
    readtime: 6
  -->
  
  <!-- Header/Navigation -->
  <header>
    <nav>
      <a href="/" class="nav-logo">
        <img src="/assets/images/minjae.png" alt="Minjae Cho">
        <span>Minjae Cho</span>
      </a>
      
      <div class="nav-links">
        <a href="/life/">Life</a>
        <a href="/review/" class="active">Review</a>
        <a href="/portfolio/">Portfolio</a>
        <a href="/archive/">Archive</a>
        <a href="/about/">About</a>
      </div>
      
      <div class="hamburger" aria-label="메뉴 열기" tabindex="0">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </nav>
  </header>
  
  <!-- Post Content -->
  <article class="post">
    <div class="container">
      <h1>LG AI Exaone deep 사용 후기</h1>
      
      <div class="meta">
        <div>
          <svg width="16" height="16" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
          </svg>
          March 23, 2025
        </div>
        
        <div>
          <svg width="16" height="16" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
          </svg>
          6 min read
        </div>
        
        <div>
          <svg width="16" height="16" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" />
          </svg>
          <a href="/review/">Review</a>
        </div>
      </div>
      
      <div class="post-tags">
        <a href="/tags/AI/" class="post-tag">AI</a>
        <a href="/tags/LLM/" class="post-tag">LLM</a>
        <a href="/tags/리뷰/" class="post-tag">리뷰</a>
        <a href="/tags/Korean-LLM/" class="post-tag">Korean-LLM</a>
      </div>
      
      <div class="post-content">
        <img src="/assets/images/portfolio/llm.png" alt="Exaone Deep 모델" class="post-image">
        
        <p>최근에 LG AI Research에서 나온 모델인 Exaone 3.5가 있었다.<br>
        그리고 얼마전에 또 추론이 가능한 모델로 Exaone deep이 출시되었다.</p>
        
        <img src="https://velog.velcdn.com/images/sgt-cho/post/d396af06-2771-4d57-bdb3-8b7a37d1ae03/image.png" alt="Exaone Deep" loading="lazy">
        <p class="caption">출처: <a href="https://www.lgresearch.ai/exaone" target="_blank">https://www.lgresearch.ai/exaone</a></p>
        
        <img src="https://velog.velcdn.com/images/sgt-cho/post/1406a521-9de1-4fbe-845f-091fe04a78e6/image.png" alt="Exaone Deep Interface" loading="lazy">
        
        <p>모델 파라미터 수는 2.4B, 7.8B, 32B가 있었다.<br>
        내가 가지고 있는 환경에서, 돌릴 수 있는 거는 2.4B랑 7.8B는 무리없이 돌릴 수 있지만, 32B 모델의 경우에는 양자화가 필요하다.</p>
        
        <h2>성능 비교</h2>
        
        <table>
          <thead>
            <tr>
              <th>Models</th>
              <th>MATH-500 (pass@1)</th>
              <th>AIME 2024 (pass@1/cons@6)</th>
              <th>AIME 2025 (pass@1/cons@6)</th>
              <th>CSAT Math 2025 (pass@1)</th>
              <th>GPQA Diamond (pass@1)</th>
              <th>Live Code Bench</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>EXAONE Deep 32B</strong></td>
              <td>79.5</td>
              <td>72.1 / 90.0</td>
              <td>73.7 / 92.6</td>
              <td>84.5</td>
              <td>66.1</td>
              <td>59.5</td>
            </tr>
            <tr>
              <td><strong>DeepSeek-R1-Distill-Qwen-32B</strong></td>
              <td>71.3</td>
              <td>71.2 / 89.3</td>
              <td>68.5 / 89.2</td>
              <td>83.4</td>
              <td>65.9</td>
              <td>56.3</td>
            </tr>
            <tr>
              <td><strong>QwQ-32B</strong></td>
              <td>68.9</td>
              <td>63.2 / 85.9</td>
              <td>65.3 / 87.6</td>
              <td>81.0</td>
              <td>62.5</td>
              <td>54.8</td>
            </tr>
            <tr>
              <td><strong>DeepSeek-R1 (671B)</strong></td>
              <td>74.3</td>
              <td>67.6 / 86.7</td>
              <td>66.4 / 86.2</td>
              <td>83.1</td>
              <td>62.9</td>
              <td>53.7</td>
            </tr>
            <tr>
              <td><strong>EXAONE Deep 7.88B</strong></td>
              <td>49.4</td>
              <td>70.0 / 83.9</td>
              <td>59.6 / 76.7</td>
              <td>74.2</td>
              <td>59.7</td>
              <td>47.6</td>
            </tr>
            <tr>
              <td><strong>DeepSeek-R1-Distill-Qwen-7B</strong></td>
              <td>49.7</td>
              <td>63.5 / 78.5</td>
              <td>56.4 / 73.8</td>
              <td>66.3</td>
              <td>52.6</td>
              <td>46.9</td>
            </tr>
            <tr>
              <td><strong>DeepSeek-R1-Distill-Llama-8B</strong></td>
              <td>47.1</td>
              <td>64.6 / 80.0</td>
              <td>59.3 / 76.4</td>
              <td>65.7</td>
              <td>52.2</td>
              <td>45.9</td>
            </tr>
            <tr>
              <td><strong>OpenAI-1 mini</strong></td>
              <td>46.9</td>
              <td>61.3 / 77.1</td>
              <td>59.3 / 74.9</td>
              <td>64.2</td>
              <td>50.1</td>
              <td>44.8</td>
            </tr>
            <tr>
              <td><strong>EXAONE Deep 2.48B</strong></td>
              <td>29.2</td>
              <td>59.3 / 72.3</td>
              <td>37.3 / 64.9</td>
              <td>56.7</td>
              <td>39.2</td>
              <td>32.1</td>
            </tr>
            <tr>
              <td><strong>DeepSeek-R1-Distill-Qwen-1.5B</strong></td>
              <td>28.4</td>
              <td>55.1 / 70.0</td>
              <td>34.3 / 62.5</td>
              <td>52.2</td>
              <td>38.9</td>
              <td>29.7</td>
            </tr>
          </tbody>
        </table>
        
        <p>성능은 아주 준수하게 나온다. 물론 특정 분야긴 하지만, 그래도 엄청 높은 수준이라는 것은 확실하다.<br>
        도표출처: <a href="https://github.com/LG-AI-EXAONE/EXAONE-Deep?tab=readme-ov-file" target="_blank">https://github.com/LG-AI-EXAONE-Deep</a></p>
        
        <p><a href="https://www.youtube.com/@AZisk" target="_blank">https://www.youtube.com/@AZisk</a>의 유튜브를 참고해보면<br>
        <a href="https://llm-inference-calculator-rki02.kinsta.page/" target="_blank">https://llm-inference-calculator-rki02.kinsta.page/</a> 
        llm을 돌릴 수 있는 Vram을 계산해주는 홈페이지가 있다.</p>
        
        <img src="https://velog.velcdn.com/images/sgt-cho/post/7e3c481d-cc3d-496d-bef5-3a05e0d54314/image.png" alt="LLM Inference Calculator" loading="lazy">
        
        <p>32B 모델을 돌리기에 FP16(Mac 환경이기 때문)으로는 약간 모자라다. 그래서 Q8 정도로 양자화를 진행하고, 
        해당 모델을 llama.cpp를 이용해서, ollama에 설치하고, open-webui에 올려 사용해보는 후기를 올리도록 하겠다.</p>
        
        <h2>양자화 과정</h2>
        
        <p>일단 양자화의 경우에는 허깅페이스를 확인해보니,</p>
        <img src="https://velog.velcdn.com/images/sgt-cho/post/79b56176-c47b-4352-8f2c-a0850a1a1b78/image.png" alt="Huggingface GGUF" loading="lazy">
        
        <p>이미 양자화가 되어있는 gguf 파일들이 있어서, 그냥 Q8 모델을 다운받아서 진행했습니다.</p>
        
        <p>그리고 현재 글 작성 시점에는 ollama에 exaone-deep이 출시가 되어있습니다.</p>
        <p><a href="https://ollama.com/library/exaone-deep" target="_blank">https://ollama.com/library/exaone-deep</a></p>
        
        <p>제가 진행할 때에는 없어서 직접 다운받아서 했는데, 새로 하실분들은 ollama에서 사용하실거라면 해당 사이트에서 하시는게 편할 것 같습니다.</p>
        
        <p>아무튼 제가 진행한건 이제, Q8로 양자화된 모델을 llama.cpp 이용해서 사용하는 방식이었습니다.</p>
        
        <h2>실제 사용 경험</h2>
        
        <p>이제 open-webui에서 사용을 해보면,</p>
        <img src="https://velog.velcdn.com/images/sgt-cho/post/a0adda72-9d56-41aa-92ab-30419bb5e1e3/image.png" alt="Open WebUI Demo 1" loading="lazy">
        <img src="https://velog.velcdn.com/images/sgt-cho/post/89b87c31-25fd-41b4-bbb6-6f26fdddea84/image.png" alt="Open WebUI Demo 2" loading="lazy">
        
        <p>좋은 것 같다. 속도야 뭐 필자가 mac 환경이라 느린 부분이 있으니까 어쩔 수 없고, 모델 문제는 아니다.<br>
        아무튼 한국에서 출시한 모델인 만큼 한국어도 잘 하는 느낌이 들고, 여러 분야에서 사용하기 좋을 것 같다는 생각이다.</p>
        
        <h2>결론</h2>
        
        <p>Exaone Deep은 특히 32B 모델의 경우 뛰어난 성능을 보여주고 있으며, 특히 수학과 코딩 관련 벤치마크에서 높은 점수를 기록했습니다. 
        한국어 모델로서의 강점을 가지고 있어 국내 AI 연구와 개발에 많은 도움이 될 것으로 기대됩니다.</p>
        
        <p>양자화를 통해 적은 컴퓨팅 자원으로도 충분히 활용할 수 있다는 점은 개인 연구자나 소규모 개발팀에게도 좋은 소식입니다. 
        앞으로 Exaone 시리즈가 어떻게 발전할지 기대가 됩니다.</p>
      </div>
    </div>
  </article>
  
  <!-- Footer -->
  <footer>
    <div class="container">
      <p>&copy; 2025 Minjae Cho · Built with passion and code</p>
    </div>
  </footer>
  
  <!-- Scripts -->
  <script src="/js/main.js"></script>
</body>
</html>